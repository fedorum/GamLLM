Build an LLM from scratch (i.e. ChatGPT model)

Chapter 1 - Understanding LLMs

- LLMs 'understand' human language in that they can generate text that appears coherent/ contextually relevant

- The 'large' in LLM refers to the large number of parameters and size of the dataset the model is trained on
- LLMs have a 'transformer' architecture, which allows them to pay 'selective' attention to parts of an input
- LLMs fall under the category of deep learning, which is a subset of machine learning; a subset of AI
- LLMs are trained on datasets that allow them to classify things (unlike manual rule-setting)

- LLMs are best used for tasks that involve parsing and generating text in specialised fields like medicine/ law

- Most modern LLMs are implemented using PyTorch - domain specific ones can outperform general ones like ChatGPT
- Custom LLMs are also smaller scale and can be deployed from laptops/ phones, where biggers ones are more costly
- Creating an LLM involves 2 phases - pre-training (using large datasets) and fine-tuning (using narrower datasets)
- Pre-training uses raw, unlabelled text, and gives the LLM some simple capabilities like text completion
- Fine-tuning can be 'instruction' or 'classification', which uses 'QnA' style data or 'labelled' data respectively

- The original transformer architecture was developed to translate English into German and French texts
- Transformers consist of:
    - An 'encoder' that processes the input text into numerical representations
    - A 'decoder' that processes the numerical representations and generates output text
- LLMs have a 'self-attention' mechanism that allows the model to weight the importance of different parts of an input
- Generative pre-trained transformers (GPT, like ChatGPT) use this mechanism to perform:
    - Zero shot learning, which are tasks that are carried out without any prior examples
    - Few shot learning, which involve some examples the user provides as input

- Pre-trained models of current ChatGPT versions are versatile and good for being fine-tuned for specific purposes

- GPT models were pre-trained on a next-word prediction task, which predicts the next word in a text based on the previous words
- This training is a form of self-labeling, where the structure of the data itself is the label (i.e. the predicted word)
- GPT architecture actually only contains the 'decoder' part of the transformer, AKA 'autoregressive' models
    - These models incorporate their previous outputs as inputs for future predictions
- GPT models are also interesting in that they can perform tasks that they were not trained for
    - Language translation is an 'emergent' capability of GPT, showing that diverse tasks do not require diverse models

- Building an LLM first requires data preparation and implementing the architecture (both can be done low-cost)

Chapter 2 - Working with Text data

1. Prepare the data
- Words in input cannot be processed 


