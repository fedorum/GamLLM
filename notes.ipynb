{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7898624",
   "metadata": {},
   "source": [
    "### Build an LLM from scratch (i.e. ChatGPT model)\n",
    "\n",
    "**Chapter 1 - Understanding LLMs**\n",
    "\n",
    "- LLMs 'understand' human language in that they can generate text that appears coherent/ contextually relevant<br>\n",
    "<br>\n",
    "\n",
    "- The 'large' in LLM refers to the large number of parameters and size of the dataset the model is trained on\n",
    "- LLMs have a 'transformer' architecture, which allows them to pay 'selective' attention to parts of an input\n",
    "- LLMs fall under the category of deep learning, which is a subset of machine learning; a subset of AI\n",
    "- LLMs are trained on datasets that allow them to classify things (unlike manual rule-setting)<br>\n",
    "<br>\n",
    "\n",
    "- LLMs are best used for tasks that involve parsing and generating text in specialised fields like medicine/ law<br>\n",
    "<br>\n",
    "\n",
    "- Most modern LLMs are implemented using PyTorch - domain specific ones can outperform general ones like ChatGPT\n",
    "- Custom LLMs are also smaller scale and can be deployed from laptops/ phones, where biggers ones are more costly\n",
    "- Creating an LLM involves 2 phases - pre-training (using large datasets) and fine-tuning (using narrower datasets)\n",
    "- Pre-training uses raw, unlabelled text, and gives the LLM some simple capabilities like text completion\n",
    "- Fine-tuning can be 'instruction' or 'classification', which uses 'QnA' style data or 'labelled' data respectively<br>\n",
    "<br>\n",
    "\n",
    "- The original transformer architecture was developed to translate English into German and French texts\n",
    "- Transformers consist of:\n",
    "    - An 'encoder' that processes the input text into numerical representations\n",
    "    \n",
    "    - A 'decoder' that processes the numerical representations and generates output text\n",
    "- LLMs have a 'self-attention' mechanism that allows the model to weight the importance of different parts of an input\n",
    "- Generative pre-trained transformers (GPT, like ChatGPT) use this mechanism to perform:\n",
    "    - Zero shot learning, which are tasks that are carried out without any prior examples\n",
    "    \n",
    "    - Few shot learning, which involve some examples the user provides as input<br>\n",
    "<br>\n",
    "\n",
    "- Pre-trained models of current ChatGPT versions are versatile and good for being fine-tuned for specific purposes<br>\n",
    "<br>\n",
    "\n",
    "- GPT models were pre-trained on a next-word prediction task, which predicts the next word in a text based on the previous words\n",
    "- This training is a form of self-labeling, where the structure of the data itself is the label (i.e. the predicted word)\n",
    "- GPT architecture actually only contains the 'decoder' part of the transformer, AKA 'autoregressive' models\n",
    "    - These models incorporate their previous outputs as inputs for future predictions\n",
    "- GPT models are also interesting in that they can perform tasks that they were not trained for\n",
    "    - Language translation is an 'emergent' capability of GPT, showing that diverse tasks do not require diverse models<br>\n",
    "<br>\n",
    "\n",
    "- Building an LLM first requires data preparation and implementing the architecture (both can be done low-cost)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5049ca7d",
   "metadata": {},
   "source": [
    "**Chapter 2 - Working with Text data**\n",
    "\n",
    "- Pre-training the LLM involves preparing text data by splitting it into individual word and subword 'tokens'\n",
    "- These are then encoded into vector representations (i.e. lists containing numbers)<br>\n",
    "<br>\n",
    "\n",
    "- Text embedding is the process of converting text into numerical vectors (done so as LLMs cannot process raw text)\n",
    "- Word embeddings can have more than 1 dimension (i.e. more than 1 number in a list), more dimensions = more computation<br>\n",
    "<br>\n",
    "\n",
    "- Tokenizing is the process of splitting text into tokens, where each word or punctuation is a single tokens<br>\n",
    "<br>\n",
    "\n",
    "The following code reads in a short story, \"The Verdict\", to be used as text data for the tokenization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd381b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "First 100 characters: I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "print(\"First 100 characters: \" + str(raw_text[:99]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfaa75f",
   "metadata": {},
   "source": [
    "1. Split the text into singular words and punctuations (i.e. singular tokens)\n",
    "\n",
    "- Python has a regular expression library that can be used to split text into singular words, as seen in the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cff32dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 4690\n",
      "First 10 individual words: ['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "print(\"Total number of tokens: \" + str(len(preprocessed)))\n",
    "print(\"First 10 individual words: \" + str(preprocessed[:10]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a67773",
   "metadata": {},
   "source": [
    "2. Convert the tokens (retrieved words and punctuations) into token IDs\n",
    "\n",
    "- The tokens are stored in an alphabetical vocabulary, where each token has a unique ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70906698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocab size: 1130\n",
      "Tokens 21 - 25 of the vocab:\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(\"Total vocab size: \" + str(vocab_size))\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "print(\"Tokens 21 - 25 of the vocab:\")\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    if i > 20 and i <= 25:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e82d3b1",
   "metadata": {},
   "source": [
    "3. Apply the vocabulary to convert new text data into tokens\n",
    "\n",
    "- This models the encode and decode processes of a transformer, which can be carried out by a tokenizer class\n",
    "\n",
    "- However, this will only be able to tokenize text that is within the vocabulary (duh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b551e9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ids of the words in the text: [1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "Decoded result of the tokenised words: \" It ' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\\\])', r'\\1', text)\n",
    "\n",
    "        return text\n",
    "    \n",
    "tokenizer = Tokenizer(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "       Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "\n",
    "print(\"Token ids of the words in the text: \" + str(ids))\n",
    "print(\"Decoded result of the tokenised words: \" + str(tokenizer.decode(ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a7b741",
   "metadata": {},
   "source": [
    "4. Modify the tokeniser to handle unknown words\n",
    "\n",
    "- Tokens can be added to the vocabulary to represent unknown words and text separations\n",
    "\n",
    "- The encode function of the tokeniser class can be modified to tokenise these special cases in the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84992b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New amount of tokens in vocab: 1132\n",
      "Joined sample text with separation: Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "Decoded encoded text: <|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "\n",
    "print(\"New amount of tokens in vocab: \" + str(len(vocab.items())))\n",
    "\n",
    "def new_encode(self, text):\n",
    "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "    preprocessed = [item if item in self.str_to_int\n",
    "                    else \"<|unk|>\" for item in preprocessed]\n",
    "\n",
    "    ids = [self.str_to_int[s] for s in preprocessed]\n",
    "    return ids\n",
    "\n",
    "Tokenizer.encode = new_encode\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(\"Joined sample text with separation: \" + text)\n",
    "\n",
    "new_tokeniser = Tokenizer(vocab)\n",
    "\n",
    "print(\"Decoded encoded text: \" + tokenizer.decode(tokenizer.encode(text)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
