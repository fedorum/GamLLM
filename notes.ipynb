{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7898624",
   "metadata": {},
   "source": [
    "## Build an LLM from scratch (i.e. ChatGPT model)\n",
    "\n",
    "### Chapter 1 - Understanding LLMs**\n",
    "\n",
    "- LLMs 'understand' human language in that they can generate text that appears coherent/ contextually relevant<br>\n",
    "<br>\n",
    "\n",
    "- The 'large' in LLM refers to the large number of parameters and size of the dataset the model is trained on\n",
    "- LLMs have a 'transformer' architecture, which allows them to pay 'selective' attention to parts of an input\n",
    "- LLMs fall under the category of deep learning, which is a subset of machine learning; a subset of AI\n",
    "- LLMs are trained on datasets that allow them to classify things (unlike manual rule-setting)<br>\n",
    "<br>\n",
    "\n",
    "- LLMs are best used for tasks that involve parsing and generating text in specialised fields like medicine/ law<br>\n",
    "<br>\n",
    "\n",
    "- Most modern LLMs are implemented using PyTorch - domain specific ones can outperform general ones like ChatGPT\n",
    "- Custom LLMs are also smaller scale and can be deployed from laptops/ phones, where biggers ones are more costly\n",
    "- Creating an LLM involves 2 phases - pre-training (using large datasets) and fine-tuning (using narrower datasets)\n",
    "- Pre-training uses raw, unlabelled text, and gives the LLM some simple capabilities like text completion\n",
    "- Fine-tuning can be 'instruction' or 'classification', which uses 'QnA' style data or 'labelled' data respectively<br>\n",
    "<br>\n",
    "\n",
    "- The original transformer architecture was developed to translate English into German and French texts\n",
    "- Transformers consist of:\n",
    "    - An 'encoder' that processes the input text into numerical representations\n",
    "    \n",
    "    - A 'decoder' that processes the numerical representations and generates output text\n",
    "- LLMs have a 'self-attention' mechanism that allows the model to weight the importance of different parts of an input\n",
    "- Generative pre-trained transformers (GPT, like ChatGPT) use this mechanism to perform:\n",
    "    - Zero shot learning, which are tasks that are carried out without any prior examples\n",
    "\n",
    "    - Few shot learning, which involve some examples the user provides as input<br>\n",
    "<br>\n",
    "\n",
    "- Pre-trained models of current ChatGPT versions are versatile and good for being fine-tuned for specific purposes<br>\n",
    "<br>\n",
    "\n",
    "- GPT models were pre-trained on a next-word prediction task, which predicts the next word in a text based on the previous words\n",
    "- This training is a form of self-labeling, where the structure of the data itself is the label (i.e. the predicted word)\n",
    "- GPT architecture actually only contains the 'decoder' part of the transformer, AKA 'autoregressive' models\n",
    "    - These models incorporate their previous outputs as inputs for future predictions\n",
    "- GPT models are also interesting in that they can perform tasks that they were not trained for\n",
    "    - Language translation is an 'emergent' capability of GPT, showing that diverse tasks do not require diverse models<br>\n",
    "<br>\n",
    "\n",
    "- Building an LLM first requires data preparation and implementing the architecture (both can be done low-cost)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5049ca7d",
   "metadata": {},
   "source": [
    "### Chapter 2 - Working with Text data\n",
    "\n",
    "- Pre-training the LLM involves preparing text data by splitting it into individual word and subword 'tokens'\n",
    "- These are then encoded into vector representations (i.e. lists containing numbers)<br>\n",
    "<br>\n",
    "\n",
    "- Text embedding is the process of converting text into numerical vectors (done so as LLMs cannot process raw text)\n",
    "- Word embeddings can have more than 1 dimension (i.e. more than 1 number in a list), more dimensions = more computation<br>\n",
    "<br>\n",
    "\n",
    "- tokenising is the process of splitting text into tokens, where each word or punctuation is a single tokens<br>\n",
    "<br>\n",
    "\n",
    "The following code reads in a short story, \"The Verdict\", to be used as text data for the tokenisation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd381b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "\n",
      "First 100 characters: I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "print(\"\\nFirst 100 characters: \" + str(raw_text[:99]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfaa75f",
   "metadata": {},
   "source": [
    "1. Split the text into singular words and punctuations (i.e. singular tokens)\n",
    "\n",
    "- Python has a regular expression library that can be used to split text into singular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cff32dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 4690\n",
      "\n",
      "First 10 individual words: ['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "print(\"Total number of tokens: \" + str(len(preprocessed)))\n",
    "print(\"\\nFirst 10 individual words: \" + str(preprocessed[:10]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a67773",
   "metadata": {},
   "source": [
    "2. Convert the tokens (retrieved words and punctuations) into token IDs\n",
    "\n",
    "- The tokens are stored in an alphabetical vocabulary, where each token has a unique ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70906698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocab size: 1130\n",
      "\n",
      "Tokens 21 - 25 of the vocab:\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(\"Total vocab size: \" + str(vocab_size))\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "print(\"\\nTokens 21 - 25 of the vocab:\")\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    if i > 20 and i <= 25:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e82d3b1",
   "metadata": {},
   "source": [
    "3. Apply the vocabulary to convert new text data into tokens\n",
    "\n",
    "- This models the encode and decode processes of a transformer, which can be carried out by a tokeniser class\n",
    "\n",
    "- However, this will only be able to tokenise text that is within the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b551e9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ids of the words in the text: [1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "\n",
      "Decoded result of the tokenised words: \" It ' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "class Tokeniser:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\\\])', r'\\1', text)\n",
    "\n",
    "        return text\n",
    "    \n",
    "tokeniser = Tokeniser(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "       Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokeniser.encode(text)\n",
    "\n",
    "print(\"Token ids of the words in the text: \" + str(ids))\n",
    "print(\"\\nDecoded result of the tokenised words: \" + str(tokeniser.decode(ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a7b741",
   "metadata": {},
   "source": [
    "4. Modify the tokeniser to handle unknown words\n",
    "\n",
    "- Tokens can be added to the vocabulary to represent unknown words and text separations\n",
    "\n",
    "- The encode function of the tokeniser class can be modified to tokenise these special cases in the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84992b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New amount of tokens in vocab: 1132\n",
      "\n",
      "Joined sample text with separation: Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "\n",
      "Decoded encoded text: <|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "\n",
    "print(\"New amount of tokens in vocab: \" + str(len(vocab.items())))\n",
    "\n",
    "def new_encode(self, text):\n",
    "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "    preprocessed = [item if item in self.str_to_int\n",
    "                    else \"<|unk|>\" for item in preprocessed]\n",
    "\n",
    "    ids = [self.str_to_int[s] for s in preprocessed]\n",
    "    return ids\n",
    "\n",
    "Tokeniser.encode = new_encode\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(\"\\nJoined sample text with separation: \" + text)\n",
    "\n",
    "new_tokeniser = Tokeniser(vocab)\n",
    "\n",
    "print(\"\\nDecoded encoded text: \" + new_tokeniser.decode(new_tokeniser.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fa9149",
   "metadata": {},
   "source": [
    "*Unknown words can also be handled by tokenising them through Byte Pair Encoding (BPE)*\n",
    "\n",
    "- A BPE tokeniser breaks down unknown words into subwords which exist within the vocab and have token ids\n",
    "\n",
    "- On subsequent iterations, the tokeniser merges frequent characters into larger words, which increase the vocab\n",
    "\n",
    "*This was used to train models like GPT-2 and GPT-3, the original models for ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cddac73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed tiktoken version:  0.12.0\n",
      "\n",
      "Tiktoken tokeniser encoded ids: [15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
      "\n",
      "Tiktoken tokeniser decoded words: Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "\n",
    "print(\"Installed tiktoken version: \", version(\"tiktoken\"))\n",
    "\n",
    "tokeniser = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "    \"of someunknownPlace.\"\n",
    ")\n",
    "ids = tokeniser.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(\"\\nTiktoken tokeniser encoded ids: \" + str(ids))\n",
    "\n",
    "words = tokeniser.decode(ids)\n",
    "\n",
    "print(\"\\nTiktoken tokeniser decoded words: \" + words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbf2166",
   "metadata": {},
   "source": [
    "5. Generate input-target pairs from the text data using a 'sliding window'\n",
    "\n",
    "- LLMs are pretrained by predicting the next word in a text, where the predicted words are taken in as input each time\n",
    "\n",
    "- The 'sliding window' takes a group of words in a text for each prediction (i.e. a single context), and moves across it to continue predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c210b894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the verdict, as tokenised by the BPE tokeniser: 5145\n",
      "\n",
      "Illustration of how the sliding window works:\n",
      "x:  [290, 4920, 2241, 287]\n",
      "y:       [4920, 2241, 287, 257]\n",
      "\n",
      " and  --->  established\n",
      " and established  --->  himself\n",
      " and established himself  --->  in\n",
      " and established himself in  --->  a\n"
     ]
    }
   ],
   "source": [
    "the_verdict = raw_text\n",
    "enc_text = tokeniser.encode(the_verdict)\n",
    "\n",
    "print(\"Number of tokens in the verdict, as tokenised by the BPE tokeniser: \" + str(len(enc_text)))\n",
    "\n",
    "enc_sample = enc_text[50:]\n",
    "\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1: context_size + 1]\n",
    "\n",
    "print(\"\\nIllustration of how the sliding window works:\")\n",
    "print(f\"x:  {x}\")\n",
    "print(f\"y:       {y}\\n\")\n",
    "\n",
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample [i]\n",
    "    print(tokeniser.decode(context), \" --->\", tokeniser.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1fc6d9",
   "metadata": {},
   "source": [
    "6. Implement a more efficient way of iterating over the text data and returning the input-target pairs\n",
    "\n",
    "- PyTorch is a library that can return data as 'tensors' (i.e. multidimensional arrays)\n",
    "\n",
    "- The input tensor contains the text data the LLM sees and the target tensor contains the targets the LLM predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "855f67d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python310\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First input and target of the LLM: [tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "\n",
      "Second input and target of the LLM: [tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# creating a class to initialise a text dataset\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, txt, tokeniser, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokeniser.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1:i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]\n",
    "    \n",
    "# creating a function to create a dataloader\n",
    "def create_dataloader(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    tokeniser = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDataset(txt, tokeniser, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "dataloader = create_dataloader(the_verdict, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "\n",
    "print(\"First input and target of the LLM: \" + str(first_batch))\n",
    "\n",
    "second_batch = next(data_iter)\n",
    "\n",
    "print(\"\\nSecond input and target of the LLM: \" + str(second_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872a4183",
   "metadata": {},
   "source": [
    "7. Convert the token IDs into embeddings (i.e. random values that give a weight to each token)\n",
    "\n",
    "- Each token in the vocab is assigned a number of random output dimensions (3 for this example)\n",
    "\n",
    "- When any token is read in as input, these output dimensions will be looked up in the vocab (i.e. embedding layer) and returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "703f9de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings of tokens in vocab:\n",
      " Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n",
      "\n",
      "Embeddings for token 3/ row 4 in layer:\n",
      " tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Embeddings for tokens in input_ids:\n",
      " tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "# pytorch adds random values to each of the output dimensions of each token\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "print(\"Embeddings of tokens in vocab:\\n\", embedding_layer.weight)\n",
    "\n",
    "print(\"\\nEmbeddings for token 3/ row 4 in layer:\\n\", embedding_layer(torch.tensor([3])))\n",
    "\n",
    "print(\"\\nEmbeddings for tokens in input_ids:\\n\", embedding_layer(input_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c700b6c",
   "metadata": {},
   "source": [
    "8. Change the token embeddings to account for different positions in a text input\n",
    "\n",
    "- When a text input has duplicate words, the same token embeddings are returned for those 2 words (this is wrong)\n",
    "\n",
    "- To avoid this, the original token embeddings can be modified to indicate their positions (absolute positional embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c701108c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n",
      "\n",
      "Token embeddings shape:\n",
      " torch.Size([8, 4, 256])\n",
      "\n",
      "Position embeddings shape:\n",
      " torch.Size([4, 256])\n",
      "\n",
      "Final input embeddings shape:\n",
      " torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# using the size and dimensions of the BPE tokeniser as an example\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "# using 'the-verdict' text as the text input for the dataloader\n",
    "max_length = 4\n",
    "dataloader = create_dataloader(the_verdict, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "# each 'batch' corresponds to the number of rows and the 'max_length' is the number of tokens in each row\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)\n",
    "\n",
    "# this adds 256 random values to each of the token ids (the gpt-3 model added 12,288 :O)\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "\n",
    "print(\"\\nToken embeddings shape:\\n\", token_embeddings.shape)\n",
    "\n",
    "# to modify the embeddings to indicate their position, a new layer must be created\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "\n",
    "print(\"\\nPosition embeddings shape:\\n\", pos_embeddings.shape)\n",
    "\n",
    "# add the 2 layers together to create the final input embeddings for the LLM\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "\n",
    "print(\"\\nFinal input embeddings shape:\\n\", input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0be5c8",
   "metadata": {},
   "source": [
    "The embeddings can be thought of as layers, where the:\n",
    "\n",
    "- First layer: contains multiple input batches\n",
    "\n",
    "- Second layer: each input batch contains multiple tokens\n",
    "\n",
    "- Third layer: each token contains multiple embeddings (256 in the code above)\n",
    "\n",
    "**The initial text data is now prepared for processing by the main LLM modules!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8086dc0f",
   "metadata": {},
   "source": [
    "### Chapter 3 - Coding Attention Mechanisms\n",
    "\n",
    "- Attention mechanisms are part of the LLM architecture, and take token embeddings as input\n",
    "\n",
    "- These can be implemented iteratively, going from a simplified -> normal -> causal -> multi-head<br>\n",
    "<br>\n",
    "\n",
    "- Pre-LLM architectures did not have attention mechanisms and could not process long sequences of text\n",
    "\n",
    "- They could not process the grammatical structure and context of text, affecting capabilites like word translation\n",
    "\n",
    "- An encoder-decoder recurrent neural network (RNN) can be used to address this problem:\n",
    "    1. The encoder takes in input text and gives it a 'hidden state', which carries the context for the sentence\n",
    "\n",
    "    2. The hidden state of the text is updated as each word is processed, eventually creating a final hidden state\n",
    "\n",
    "    3. The decoder takes the final hidden state to translate the sentence one word at a time\n",
    "\n",
    "- However, encoder-decoder RNNs cannot access earlier hidden states (only the final one), causing a loss of context\n",
    "\n",
    "- This drove the development of attention mechanisms, which can 'pay attention' to all tokens and contexts of an input<br>\n",
    "<br>\n",
    "\n",
    "- The transformer architecture uses an attention mechanism similar to the Bahdanau mechanism for RNNs\n",
    "\n",
    "- The mechanism can access all input tokens selectively and give some more weight than others, affecting output\n",
    "\n",
    "- In essence, 'self-attention' allows each word in the input to consider the positions of all the other words<br>\n",
    "<br>\n",
    "\n",
    "- Implementing a simplified attention mechanism starts with a sample text input with a smaller embedding dimension (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95c22172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# the inputs are token embeddings for the tokens \"Your, journey, starts, with, one, step\"\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89],\n",
    "     [0.55, 0.87, 0.66],\n",
    "     [0.57, 0.85, 0.64],\n",
    "     [0.22, 0.58, 0.33],\n",
    "     [0.77, 0.25, 0.10],\n",
    "     [0.05, 0.80, 0.55]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a1f3bc",
   "metadata": {},
   "source": [
    "1. Calculate the attention scores of each of the tokens in relation to one of the tokens (i.e. the second one)\n",
    "\n",
    "- The dot product between the token embeddings of the second token and that of the other tokens is calculated\n",
    "\n",
    "- This represents how closely two vectors are aligned, where a higher dot product = higher attention between two tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df80ea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores for each of the 6 tokens:\n",
      " tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # query token is the second token\n",
    "attn_scores_2 = torch.empty(inputs.shape[0]) # taking the number of tokens from the inputs (i.e. 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "\n",
    "print(\"Attention scores for each of the 6 tokens:\\n\", attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1cf917",
   "metadata": {},
   "source": [
    "2. Normalise each of the attention scores so that they add up to 1\n",
    "\n",
    "- This is a convention that is useful for interpretation and maintaining training stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a436c6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n",
      "\n",
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights: \" + str(attn_weights_2))\n",
    "print(\"Sum: \" + str(attn_weights_2.sum()))\n",
    "\n",
    "# the pytorch function manages more extreme values\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(\"\\nAttention weights: \" + str(attn_weights_2))\n",
    "print(\"Sum: \" + str(attn_weights_2.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d30e268",
   "metadata": {},
   "source": [
    "3. Calculate the context vector for the second token\n",
    "\n",
    "- This multiplies the token embeddings with their corresponding attention weights and summing the result\n",
    "\n",
    "- Context vectors provide an enriched representation of each token in relation to other tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "436d54df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector for second token: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i] * x_i\n",
    "\n",
    "print(\"Context vector for second token: \" + str(context_vec_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4005da4e",
   "metadata": {},
   "source": [
    "4. Replicate this process for all other tokens (tokens 1, 3 - 6)\n",
    "\n",
    "- Compute the attention scores, normalise them to attention weights, and compute them in context vectors\n",
    "\n",
    "- The 'dim' parameter of the softmax function denotes the dimension that the normalisation will take place:\n",
    "    - A parameter of '-1' simply refers to the last dimension of the tensor (like where -1 refers to the last element in a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3296aeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores for each of the tokens in relation to the others:\n",
      " tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "\n",
      "Norrmalised attention weights:\n",
      " tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
      "\n",
      "Context vectors for each of the tokens:\n",
      " tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "# print(\"Attention scores for each of the tokens in relation to the others:\\n\", attn_scores)\n",
    "\n",
    "# for loops are slow, the matrix multiplication function in pytorch is a faster way\n",
    "attn_scores = inputs @ inputs.T\n",
    "\n",
    "print(\"Attention scores for each of the tokens in relation to the others:\\n\", attn_scores)\n",
    "\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "print(\"\\nNorrmalised attention weights:\\n\",attn_weights)\n",
    "\n",
    "context_vecs = attn_weights @ inputs\n",
    "\n",
    "print(\"\\nContext vectors for each of the tokens:\\n\", context_vecs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
