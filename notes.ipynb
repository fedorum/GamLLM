{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7898624",
   "metadata": {},
   "source": [
    "### Build an LLM from scratch (i.e. ChatGPT model)\n",
    "\n",
    "**Chapter 1 - Understanding LLMs**\n",
    "\n",
    "- LLMs 'understand' human language in that they can generate text that appears coherent/ contextually relevant<br>\n",
    "<br>\n",
    "\n",
    "- The 'large' in LLM refers to the large number of parameters and size of the dataset the model is trained on\n",
    "- LLMs have a 'transformer' architecture, which allows them to pay 'selective' attention to parts of an input\n",
    "- LLMs fall under the category of deep learning, which is a subset of machine learning; a subset of AI\n",
    "- LLMs are trained on datasets that allow them to classify things (unlike manual rule-setting)<br>\n",
    "<br>\n",
    "\n",
    "- LLMs are best used for tasks that involve parsing and generating text in specialised fields like medicine/ law<br>\n",
    "<br>\n",
    "\n",
    "- Most modern LLMs are implemented using PyTorch - domain specific ones can outperform general ones like ChatGPT\n",
    "- Custom LLMs are also smaller scale and can be deployed from laptops/ phones, where biggers ones are more costly\n",
    "- Creating an LLM involves 2 phases - pre-training (using large datasets) and fine-tuning (using narrower datasets)\n",
    "- Pre-training uses raw, unlabelled text, and gives the LLM some simple capabilities like text completion\n",
    "- Fine-tuning can be 'instruction' or 'classification', which uses 'QnA' style data or 'labelled' data respectively<br>\n",
    "<br>\n",
    "\n",
    "- The original transformer architecture was developed to translate English into German and French texts\n",
    "- Transformers consist of:\n",
    "    - An 'encoder' that processes the input text into numerical representations\n",
    "    \n",
    "    - A 'decoder' that processes the numerical representations and generates output text\n",
    "- LLMs have a 'self-attention' mechanism that allows the model to weight the importance of different parts of an input\n",
    "- Generative pre-trained transformers (GPT, like ChatGPT) use this mechanism to perform:\n",
    "    - Zero shot learning, which are tasks that are carried out without any prior examples\n",
    "    \n",
    "    - Few shot learning, which involve some examples the user provides as input<br>\n",
    "<br>\n",
    "\n",
    "- Pre-trained models of current ChatGPT versions are versatile and good for being fine-tuned for specific purposes<br>\n",
    "<br>\n",
    "\n",
    "- GPT models were pre-trained on a next-word prediction task, which predicts the next word in a text based on the previous words\n",
    "- This training is a form of self-labeling, where the structure of the data itself is the label (i.e. the predicted word)\n",
    "- GPT architecture actually only contains the 'decoder' part of the transformer, AKA 'autoregressive' models\n",
    "    - These models incorporate their previous outputs as inputs for future predictions\n",
    "- GPT models are also interesting in that they can perform tasks that they were not trained for\n",
    "    - Language translation is an 'emergent' capability of GPT, showing that diverse tasks do not require diverse models<br>\n",
    "<br>\n",
    "\n",
    "- Building an LLM first requires data preparation and implementing the architecture (both can be done low-cost)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5049ca7d",
   "metadata": {},
   "source": [
    "**Chapter 2 - Working with Text data**\n",
    "\n",
    "- Pre-training the LLM involves preparing text data by splitting it into individual word and subword 'tokens'\n",
    "- These are then encoded into vector representations (i.e. lists containing numbers)<br>\n",
    "<br>\n",
    "\n",
    "- Text embedding is the process of converting text into numerical vectors (done so as LLMs cannot process raw text)\n",
    "- Word embeddings can have more than 1 dimension (i.e. more than 1 number in a list), more dimensions = more computation<br>\n",
    "<br>\n",
    "\n",
    "- tokenising is the process of splitting text into tokens, where each word or punctuation is a single tokens<br>\n",
    "<br>\n",
    "\n",
    "The following code reads in a short story, \"The Verdict\", to be used as text data for the tokenisation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd381b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "\n",
      "First 100 characters: I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "print(\"\\nFirst 100 characters: \" + str(raw_text[:99]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfaa75f",
   "metadata": {},
   "source": [
    "1. Split the text into singular words and punctuations (i.e. singular tokens)\n",
    "\n",
    "- Python has a regular expression library that can be used to split text into singular words, as seen in the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cff32dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 4690\n",
      "\n",
      "First 10 individual words: ['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "print(\"Total number of tokens: \" + str(len(preprocessed)))\n",
    "print(\"\\nFirst 10 individual words: \" + str(preprocessed[:10]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a67773",
   "metadata": {},
   "source": [
    "2. Convert the tokens (retrieved words and punctuations) into token IDs\n",
    "\n",
    "- The tokens are stored in an alphabetical vocabulary, where each token has a unique ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "70906698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocab size: 1130\n",
      "\n",
      "Tokens 21 - 25 of the vocab:\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(\"Total vocab size: \" + str(vocab_size))\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "print(\"\\nTokens 21 - 25 of the vocab:\")\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    if i > 20 and i <= 25:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e82d3b1",
   "metadata": {},
   "source": [
    "3. Apply the vocabulary to convert new text data into tokens\n",
    "\n",
    "- This models the encode and decode processes of a transformer, which can be carried out by a tokeniser class\n",
    "\n",
    "- However, this will only be able to tokenise text that is within the vocabulary (duh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b551e9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ids of the words in the text: [1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "\n",
      "Decoded result of the tokenised words: \" It ' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "class Tokeniser:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\\\])', r'\\1', text)\n",
    "\n",
    "        return text\n",
    "    \n",
    "tokeniser = Tokeniser(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "       Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokeniser.encode(text)\n",
    "\n",
    "print(\"Token ids of the words in the text: \" + str(ids))\n",
    "print(\"\\nDecoded result of the tokenised words: \" + str(tokeniser.decode(ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a7b741",
   "metadata": {},
   "source": [
    "4. Modify the tokeniser to handle unknown words\n",
    "\n",
    "- Tokens can be added to the vocabulary to represent unknown words and text separations\n",
    "\n",
    "- The encode function of the tokeniser class can be modified to tokenise these special cases in the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84992b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New amount of tokens in vocab: 1132\n",
      "\n",
      "Joined sample text with separation: Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "\n",
      "Decoded encoded text: <|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "\n",
    "print(\"New amount of tokens in vocab: \" + str(len(vocab.items())))\n",
    "\n",
    "def new_encode(self, text):\n",
    "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "    preprocessed = [item if item in self.str_to_int\n",
    "                    else \"<|unk|>\" for item in preprocessed]\n",
    "\n",
    "    ids = [self.str_to_int[s] for s in preprocessed]\n",
    "    return ids\n",
    "\n",
    "Tokeniser.encode = new_encode\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(\"\\nJoined sample text with separation: \" + text)\n",
    "\n",
    "new_tokeniser = Tokeniser(vocab)\n",
    "\n",
    "print(\"\\nDecoded encoded text: \" + new_tokeniser.decode(new_tokeniser.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fa9149",
   "metadata": {},
   "source": [
    "*Unknown words can also be handled by tokenising them through Byte Pair Encoding (BPE)*\n",
    "\n",
    "- A BPE tokeniser breaks down unknown words into subwords which exist within the vocab and have token ids\n",
    "\n",
    "- On subsequent iterations, the tokeniser merges frequent characters into larger words, which increase the vocab\n",
    "\n",
    "*This was used to train models like GPT-2 and GPT-3, the original models for ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cddac73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed tiktoken version:  0.12.0\n",
      "\n",
      "Tiktoken tokeniser encoded ids: [15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
      "\n",
      "Tiktoken tokeniser decoded words: Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "\n",
    "print(\"Installed tiktoken version: \", version(\"tiktoken\"))\n",
    "\n",
    "tokeniser = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "    \"of someunknownPlace.\"\n",
    ")\n",
    "ids = tokeniser.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(\"\\nTiktoken tokeniser encoded ids: \" + str(ids))\n",
    "\n",
    "words = tokeniser.decode(ids)\n",
    "\n",
    "print(\"\\nTiktoken tokeniser decoded words: \" + words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbf2166",
   "metadata": {},
   "source": [
    "5. Generate input-target pairs from the data using a 'sliding window'\n",
    "\n",
    "- LLMs are pretrained by predicting the next word in a text, where the predicted words are taken in as input each time\n",
    "\n",
    "- The 'sliding window' takes a group of words in a text for each prediction (i.e. a single context), and moves across it to continue predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c210b894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the verdict, as tokenised by the BPE tokeniser: 5145\n",
      "\n",
      "Illustration of how the sliding window works:\n",
      "x:  [290, 4920, 2241, 287]\n",
      "y:       [4920, 2241, 287, 257]\n",
      "\n",
      " and  --->  established\n",
      " and established  --->  himself\n",
      " and established himself  --->  in\n",
      " and established himself in  --->  a\n"
     ]
    }
   ],
   "source": [
    "the_verdict = raw_text\n",
    "enc_text = tokeniser.encode(the_verdict)\n",
    "\n",
    "print(\"Number of tokens in the verdict, as tokenised by the BPE tokeniser: \" + str(len(enc_text)))\n",
    "\n",
    "enc_sample = enc_text[50:]\n",
    "\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1: context_size + 1]\n",
    "\n",
    "print(\"\\nIllustration of how the sliding window works:\")\n",
    "print(f\"x:  {x}\")\n",
    "print(f\"y:       {y}\\n\")\n",
    "\n",
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample [i]\n",
    "    print(tokeniser.decode(context), \" --->\", tokeniser.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1fc6d9",
   "metadata": {},
   "source": [
    "6. Implement a more efficient way of iterating over the text data and returning the input-target pairs\n",
    "\n",
    "- PyTorch is a library that can return data as 'tensors' (i.e. multidimensional arrays)\n",
    "\n",
    "- The input tensor contains the text data the LLM sees and the target tensor contains the targets the LLM predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "855f67d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First input and target of the LLM: [tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "\n",
      "Second input and target of the LLM: [tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# creating a class to initialise a text dataset\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, txt, tokeniser, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokeniser.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1:i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]\n",
    "    \n",
    "# creating a function to create a dataloader\n",
    "def create_dataloader(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    tokeniser = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDataset(txt, tokeniser, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "dataloader = create_dataloader(the_verdict, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "\n",
    "print(\"First input and target of the LLM: \" + str(first_batch))\n",
    "\n",
    "second_batch = next(data_iter)\n",
    "\n",
    "print(\"\\nSecond input and target of the LLM: \" + str(second_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872a4183",
   "metadata": {},
   "source": [
    "7. Convert the token IDs into embeddings (i.e. random values that give a weight to each token)\n",
    "\n",
    "- Each token in the vocab is assigned a number of random output dimensions (3 for this example)\n",
    "\n",
    "- When any token is read in as input, these output dimensions will be looked up in the vocab (i.e. embedding layer) and returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "703f9de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings of tokens in vocab:\n",
      " Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n",
      "\n",
      "Embeddings for token 3/ row 4 in layer:\n",
      " tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Embeddings for tokens in input_ids:\n",
      " tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "# pytorch adds random values to each of the output dimensions of each token\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "print(\"Embeddings of tokens in vocab:\\n\", embedding_layer.weight)\n",
    "\n",
    "print(\"\\nEmbeddings for token 3/ row 4 in layer:\\n\", embedding_layer(torch.tensor([3])))\n",
    "\n",
    "print(\"\\nEmbeddings for tokens in input_ids:\\n\", embedding_layer(input_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c700b6c",
   "metadata": {},
   "source": [
    "8. Change the token embeddings to account for different positions in a text input\n",
    "\n",
    "- When a text input has duplicate words, the same token embeddings are returned for those 2 words (this is wrong)\n",
    "\n",
    "- To avoid this, the original token embeddings can be modified to indicate their positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c701108c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n",
      "\n",
      "Token embeddings shape:\n",
      " torch.Size([8, 4, 256])\n",
      "\n",
      "Position embeddings shape:\n",
      " torch.Size([4, 256])\n",
      "\n",
      "Final input embeddings shape:\n",
      " torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# using the size and dimensions of the BPE tokeniser as an example\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "# using 'the-verdict' text as the text input for the dataloader\n",
    "max_length = 4\n",
    "dataloader = create_dataloader(the_verdict, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "# each 'batch' corresponds to the number of rows and the 'max_length' is the number of tokens in each row\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)\n",
    "\n",
    "# this adds 256 random values to each of the token ids (the gpt-3 model added 12,288 :O)\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "\n",
    "print(\"\\nToken embeddings shape:\\n\", token_embeddings.shape)\n",
    "\n",
    "# to modify the embeddings to indicate their position, a new layer must be created\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "\n",
    "print(\"\\nPosition embeddings shape:\\n\", pos_embeddings.shape)\n",
    "\n",
    "# add the 2 layers together to create the final input embeddings for the LLM\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "\n",
    "print(\"\\nFinal input embeddings shape:\\n\", input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0be5c8",
   "metadata": {},
   "source": [
    "The embeddings can be thought of as layers, where the:\n",
    "\n",
    "- First layer: contains multiple input batches\n",
    "\n",
    "- Second layer: each input batch contains multiple tokens\n",
    "\n",
    "- Third layer: each token contains multiple embeddings (256 in the code above)\n",
    "\n",
    "**The initial text data is now prepared for processing by the main LLM modules!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
