{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7898624",
   "metadata": {},
   "source": [
    "# Build an LLM from scratch (i.e. ChatGPT model)\n",
    "\n",
    "## Chapter 1 - Understanding LLMs\n",
    "\n",
    "- LLMs 'understand' human language in that they can generate text that appears coherent/ contextually relevant<br>\n",
    "<br>\n",
    "\n",
    "- The 'large' in LLM refers to the large number of parameters and size of the dataset the model is trained on\n",
    "- LLMs have a 'transformer' architecture, which allows them to pay 'selective' attention to parts of an input\n",
    "- LLMs fall under the category of deep learning, which is a subset of machine learning; a subset of AI\n",
    "- LLMs are trained on datasets that allow them to classify things (unlike manual rule-setting)<br>\n",
    "<br>\n",
    "\n",
    "- LLMs are best used for tasks that involve parsing and generating text in specialised fields like medicine/ law<br>\n",
    "<br>\n",
    "\n",
    "- Most modern LLMs are implemented using PyTorch - domain specific ones can outperform general ones like ChatGPT\n",
    "- Custom LLMs are also smaller scale and can be deployed from laptops/ phones, where biggers ones are more costly\n",
    "- Creating an LLM involves 2 phases - pre-training (using large datasets) and fine-tuning (using narrower datasets)\n",
    "- Pre-training uses raw, unlabelled text, and gives the LLM some simple capabilities like text completion\n",
    "- Fine-tuning can be 'instruction' or 'classification', which uses 'QnA' style data or 'labelled' data respectively<br>\n",
    "<br>\n",
    "\n",
    "- The original transformer architecture was developed to translate English into German and French texts\n",
    "- Transformers consist of:\n",
    "    - An 'encoder' that processes the input text into numerical representations\n",
    "    \n",
    "    - A 'decoder' that processes the numerical representations and generates output text\n",
    "- LLMs have a 'self-attention' mechanism that allows the model to weight the importance of different parts of an input\n",
    "- Generative pre-trained transformers (GPT, like ChatGPT) use this mechanism to perform:\n",
    "    - Zero shot learning, which are tasks that are carried out without any prior examples\n",
    "\n",
    "    - Few shot learning, which involve some examples the user provides as input<br>\n",
    "<br>\n",
    "\n",
    "- Pre-trained models of current ChatGPT versions are versatile and good for being fine-tuned for specific purposes<br>\n",
    "<br>\n",
    "\n",
    "- GPT models were pre-trained on a next-word prediction task, which predicts the next word in a text based on the previous words\n",
    "- This training is a form of self-labeling, where the structure of the data itself is the label (i.e. the predicted word)\n",
    "- GPT architecture actually only contains the 'decoder' part of the transformer, AKA 'autoregressive' models\n",
    "    - These models incorporate their previous outputs as inputs for future predictions\n",
    "- GPT models are also interesting in that they can perform tasks that they were not trained for\n",
    "    - Language translation is an 'emergent' capability of GPT, showing that diverse tasks do not require diverse models<br>\n",
    "<br>\n",
    "\n",
    "- Building an LLM first requires data preparation and implementing the architecture (both can be done low-cost)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5049ca7d",
   "metadata": {},
   "source": [
    "## Chapter 2 - Working with Text data\n",
    "\n",
    "- Pre-training the LLM involves preparing text data by splitting it into individual word and subword 'tokens'\n",
    "- These are then encoded into vector representations (i.e. lists containing numbers)<br>\n",
    "<br>\n",
    "\n",
    "- Text embedding is the process of converting text into numerical vectors (done so as LLMs cannot process raw text)\n",
    "- Word embeddings can have more than 1 dimension (i.e. more than 1 number in a list), more dimensions = more computation<br>\n",
    "<br>\n",
    "\n",
    "- tokenising is the process of splitting text into tokens, where each word or punctuation is a single tokens<br>\n",
    "<br>\n",
    "\n",
    "The following code reads in a short story, \"The Verdict\", to be used as text data for the tokenisation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd381b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "\n",
      "First 100 characters: I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "print(\"\\nFirst 100 characters: \" + str(raw_text[:99]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfaa75f",
   "metadata": {},
   "source": [
    "### 1. Split the text into singular words and punctuations (i.e. singular tokens)\n",
    "\n",
    "- Python has a regular expression library that can be used to split text into singular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff32dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 4690\n",
      "\n",
      "First 10 individual words: ['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# using the regex library to remove characters like punctuation and brackets from the text\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "# each word counts as a single token\n",
    "print(\"Total number of tokens: \" + str(len(preprocessed)))\n",
    "print(\"\\nFirst 10 individual words/ tokens: \" + str(preprocessed[:10]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a67773",
   "metadata": {},
   "source": [
    "### 2. Convert the tokens (retrieved words and punctuations) into token IDs\n",
    "\n",
    "- The tokens are stored in an alphabetical vocabulary, where each token has a unique ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70906698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocab size: 1130\n",
      "\n",
      "Tokens 21 - 25 of the vocab:\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n"
     ]
    }
   ],
   "source": [
    "# using the sorted function to sort the words alphabetically\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(\"Total vocab size: \" + str(vocab_size))\n",
    "\n",
    "# assigning an integer to each token for the vocab\n",
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "print(\"\\nTokens 21 - 25 of the vocab:\")\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    if i > 20 and i <= 25:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e82d3b1",
   "metadata": {},
   "source": [
    "### 3. Apply the vocabulary to convert new text data into tokens\n",
    "\n",
    "- This models the encode and decode processes of a transformer, which can be carried out by a tokeniser class\n",
    "\n",
    "- However, this will only be able to tokenise text that is within the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b551e9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ids of the words in the text: [1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "\n",
      "Decoded result of the tokenised words: \" It ' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "# creating a tokeniser class that combines the previous steps of splitting and sorting the text\n",
    "class Tokeniser:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    # converts the input text into tokens for the vocab\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    # converts token ids into text\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\\\])', r'\\1', text)\n",
    "\n",
    "        return text\n",
    "\n",
    "tokeniser = Tokeniser(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "       Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokeniser.encode(text)\n",
    "\n",
    "print(\"Token ids of the words in the text: \" + str(ids))\n",
    "print(\"\\nDecoded result of the tokenised words: \" + str(tokeniser.decode(ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a7b741",
   "metadata": {},
   "source": [
    "### 4. Modify the tokeniser to handle unknown words\n",
    "\n",
    "- Tokens can be added to the vocabulary to represent unknown words and text separations\n",
    "\n",
    "- The encode function of the tokeniser class can be modified to tokenise these special cases in the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84992b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New amount of tokens in vocab: 1132\n",
      "\n",
      "Joined sample text with separation: Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "\n",
      "Decoded encoded text: <|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "# adding new tokens to the vocab to process unknown words and text separations\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "\n",
    "print(\"New amount of tokens in vocab: \" + str(len(vocab.items())))\n",
    "\n",
    "# replacing the previous encode function with a new one that replaces unknown words in the text with \"<|unk|>\"\n",
    "def new_encode(self, text):\n",
    "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "    preprocessed = [item if item in self.str_to_int\n",
    "                    else \"<|unk|>\" for item in preprocessed]\n",
    "\n",
    "    ids = [self.str_to_int[s] for s in preprocessed]\n",
    "    return ids\n",
    "\n",
    "Tokeniser.encode = new_encode\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(\"\\nJoined sample text with separation: \" + text)\n",
    "\n",
    "new_tokeniser = Tokeniser(vocab)\n",
    "\n",
    "print(\"\\nDecoded encoded text: \" + new_tokeniser.decode(new_tokeniser.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fa9149",
   "metadata": {},
   "source": [
    "*Unknown words can also be handled by tokenising them through Byte Pair Encoding (BPE)*\n",
    "\n",
    "- A BPE tokeniser breaks down unknown words into subwords which exist within the vocab and have token ids\n",
    "\n",
    "- On subsequent iterations, the tokeniser merges frequent characters into larger words, which increase the vocab\n",
    "\n",
    "*This was used to train models like GPT-2 and GPT-3, the original models for ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddac73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiktoken tokeniser encoded ids: [15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
      "\n",
      "Tiktoken tokeniser decoded words: Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# using the tiktoken library to encode and decode text\n",
    "tokeniser = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "    \"of someunknownPlace.\"\n",
    ")\n",
    "ids = tokeniser.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(\"Tiktoken tokeniser encoded ids: \" + str(ids))\n",
    "\n",
    "words = tokeniser.decode(ids)\n",
    "\n",
    "print(\"\\nTiktoken tokeniser decoded words: \" + words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbf2166",
   "metadata": {},
   "source": [
    "### 5. Generate input-target pairs from the text data using a 'sliding window'\n",
    "\n",
    "- LLMs are pretrained by predicting the next word in a text, where the predicted words are taken in as input each time\n",
    "\n",
    "- The 'sliding window' takes a group of words in a text for each prediction (i.e. a single context), and moves across it to continue predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c210b894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the verdict, as tokenised by the BPE tokeniser: 5145\n",
      "\n",
      "Illustration of how the sliding window works:\n",
      "x:  [290, 4920, 2241, 287]\n",
      "y:       [4920, 2241, 287, 257]\n",
      "\n",
      " and  --->  established\n",
      " and established  --->  himself\n",
      " and established himself  --->  in\n",
      " and established himself in  --->  a\n"
     ]
    }
   ],
   "source": [
    "the_verdict = raw_text\n",
    "enc_text = tokeniser.encode(the_verdict)\n",
    "\n",
    "print(\"Number of tokens in the verdict, as tokenised by the BPE tokeniser: \" + str(len(enc_text)))\n",
    "\n",
    "enc_sample = enc_text[50:]\n",
    "\n",
    "# the context size is the number of words the LLM will process at a single time\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1: context_size + 1]\n",
    "\n",
    "print(\"\\nIllustration of how the sliding window works:\")\n",
    "print(f\"x:  {x}\")\n",
    "print(f\"y:       {y}\\n\")\n",
    "\n",
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample [i]\n",
    "    print(tokeniser.decode(context), \" --->\", tokeniser.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1fc6d9",
   "metadata": {},
   "source": [
    "### 6. Implement a more efficient way of iterating over the text data and returning the input-target pairs\n",
    "\n",
    "- PyTorch is a library that can return data as 'tensors' (i.e. multidimensional arrays)\n",
    "\n",
    "- The input tensor contains the text data the LLM sees and the target tensor contains the targets the LLM predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855f67d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python310\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First input and target of the LLM: [tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "\n",
      "Second input and target of the LLM: [tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# creating a class to initialise a text dataset\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, txt, tokeniser, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokeniser.encode(txt)\n",
    "\n",
    "        # assigning input and target ids for the text input (the target id will be one after the input id)\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1:i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]\n",
    "    \n",
    "# creating a function to create a dataloader\n",
    "def create_dataloader(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    tokeniser = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDataset(txt, tokeniser, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "# this creates a dataloader that looks at 4 words in one iteration and predicts the next word, and shifts 4 words ahead for the next iteration\n",
    "dataloader = create_dataloader(the_verdict, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "\n",
    "print(\"First input and target of the LLM: \" + str(first_batch))\n",
    "\n",
    "second_batch = next(data_iter)\n",
    "\n",
    "print(\"\\nSecond input and target of the LLM: \" + str(second_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872a4183",
   "metadata": {},
   "source": [
    "### 7. Convert the token IDs into embeddings (i.e. random values that give a weight to each token)\n",
    "\n",
    "- Each token in the vocab is assigned a number of random output dimensions (3 for this example)\n",
    "\n",
    "- When any token is read in as input, these output dimensions will be looked up in the vocab (i.e. embedding layer) and returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "703f9de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings of tokens in vocab:\n",
      " Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n",
      "\n",
      "Embeddings for token 3/ row 4 in layer:\n",
      " tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Embeddings for the 4 tokens in input_ids:\n",
      " tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# creates a tensor/ array with 4 ids\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "# pytorch assigns random values to each of the output dimensions of each token\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "print(\"Embeddings of tokens in vocab:\\n\", embedding_layer.weight)\n",
    "\n",
    "print(\"\\nEmbeddings for token 3/ row 4 in layer:\\n\", embedding_layer(torch.tensor([3])))\n",
    "\n",
    "print(\"\\nEmbeddings for the 4 tokens in input_ids:\\n\", embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c700b6c",
   "metadata": {},
   "source": [
    "### 8. Change the token embeddings to account for different positions in a text input\n",
    "\n",
    "- When a text input has duplicate words, the same token embeddings are returned for those 2 words (this is wrong)\n",
    "\n",
    "- To avoid this, the original token embeddings can be modified to indicate their positions (absolute positional embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c701108c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n",
      "\n",
      "Token embeddings shape:\n",
      " torch.Size([8, 4, 256])\n",
      "\n",
      "Position embeddings shape:\n",
      " torch.Size([4, 256])\n",
      "\n",
      "Final input embeddings shape:\n",
      " torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# using the size and dimensions of the BPE tokeniser as an example\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "# using 'the-verdict' text as the text input for the dataloader\n",
    "max_length = 4\n",
    "dataloader = create_dataloader(the_verdict, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "# each 'batch' corresponds to the number of rows and the 'max_length' is the number of tokens in each row\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)\n",
    "\n",
    "# this adds 256 random values to each of the token ids (the gpt-3 model added 12,288 :O)\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "\n",
    "print(\"\\nToken embeddings shape:\\n\", token_embeddings.shape)\n",
    "\n",
    "# to modify the embeddings to indicate their position, a new layer must be created\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "\n",
    "print(\"\\nPosition embeddings shape:\\n\", pos_embeddings.shape)\n",
    "\n",
    "# add the 2 layers together to create the final input embeddings for the LLM\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "\n",
    "print(\"\\nFinal input embeddings shape:\\n\", input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0be5c8",
   "metadata": {},
   "source": [
    "The embeddings can be thought of as layers, where the:\n",
    "\n",
    "- First layer: contains multiple input batches\n",
    "\n",
    "- Second layer: each input batch contains multiple tokens\n",
    "\n",
    "- Third layer: each token contains multiple embeddings (256 in the code above)\n",
    "\n",
    "**The initial text data is now prepared for processing by the main LLM modules!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8086dc0f",
   "metadata": {},
   "source": [
    "## Chapter 3 - Coding Attention Mechanisms\n",
    "\n",
    "- Attention mechanisms are part of the LLM architecture, and take token embeddings as input\n",
    "\n",
    "- These can be implemented iteratively, going from a simplified -> normal -> causal -> multi-head<br>\n",
    "<br>\n",
    "\n",
    "- Pre-LLM architectures did not have attention mechanisms and could not process long sequences of text\n",
    "\n",
    "- They could not process the grammatical structure and context of text, affecting capabilites like word translation\n",
    "\n",
    "- An encoder-decoder recurrent neural network (RNN) can be used to address this problem:\n",
    "    1. The encoder takes in input text and gives it a 'hidden state', which carries the context for the sentence\n",
    "\n",
    "    2. The hidden state of the text is updated as each word is processed, eventually creating a final hidden state\n",
    "\n",
    "    3. The decoder takes the final hidden state to translate the sentence one word at a time\n",
    "\n",
    "- However, encoder-decoder RNNs cannot access earlier hidden states (only the final one), causing a loss of context\n",
    "\n",
    "- This drove the development of attention mechanisms, which can 'pay attention' to all tokens and contexts of an input<br>\n",
    "<br>\n",
    "\n",
    "- The transformer architecture uses an attention mechanism similar to the Bahdanau mechanism for RNNs\n",
    "\n",
    "- The mechanism can access all input tokens selectively and give some more weight than others, affecting output\n",
    "\n",
    "- In essence, 'self-attention' allows each word in the input to consider the positions of all the other words<br>\n",
    "<br>\n",
    "\n",
    "- Implementing a simplified attention mechanism starts with a sample text input with a smaller embedding dimension (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c22172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# the inputs are token embeddings for the tokens: \"Your\", \"journey\", \"starts\", \"with\", \"one\", \"step\"\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89],\n",
    "     [0.55, 0.87, 0.66],\n",
    "     [0.57, 0.85, 0.64],\n",
    "     [0.22, 0.58, 0.33],\n",
    "     [0.77, 0.25, 0.10],\n",
    "     [0.05, 0.80, 0.55]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a1f3bc",
   "metadata": {},
   "source": [
    "### 1. Calculate the attention scores of each of the tokens in relation to one of the tokens (i.e. the second one)\n",
    "\n",
    "- The dot product between the token embeddings of the second token and that of the other tokens is calculated\n",
    "\n",
    "- This represents how closely two vectors are aligned, where a higher dot product = higher attention between two tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df80ea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores for each of the 6 tokens:\n",
      " tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "# the query token is the token that is focused on, and is the foundation of the calculation of the scores of the other tokens\n",
    "query = inputs[1] # query token is the second token\n",
    "attn_scores_2 = torch.empty(inputs.shape[0]) # taking the number of tokens from the inputs (i.e. 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "\n",
    "print(\"Attention scores for each of the 6 tokens:\\n\", attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1cf917",
   "metadata": {},
   "source": [
    "### 2. Normalise each of the attention scores so that they add up to 1\n",
    "\n",
    "- This is a convention that is useful for interpretation and maintaining training stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a436c6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n",
      "\n",
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# taking each of the scores as a percentage of the total sum, making every one add up to 1\n",
    "attn_weights_2 = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights: \" + str(attn_weights_2))\n",
    "print(\"Sum: \" + str(attn_weights_2.sum()))\n",
    "\n",
    "# extra: the pytorch function can manage more extreme values\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(\"\\nAttention weights: \" + str(attn_weights_2))\n",
    "print(\"Sum: \" + str(attn_weights_2.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d30e268",
   "metadata": {},
   "source": [
    "### 3. Calculate the context vector for the second token\n",
    "\n",
    "- This multiplies the token embeddings with their corresponding attention weights and summing the result\n",
    "\n",
    "- Context vectors provide an enriched representation of each token in relation to other tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436d54df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector for second token: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# multiplying the attention weights of each of the tokens with their token embeddings\n",
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i] * x_i\n",
    "\n",
    "print(\"Context vector for second token: \" + str(context_vec_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4005da4e",
   "metadata": {},
   "source": [
    "### 4. Replicate this process for all other tokens (tokens 1, 3 - 6)\n",
    "\n",
    "- Compute the attention scores, normalise them to attention weights, and compute them in context vectors\n",
    "\n",
    "- The 'dim' parameter of the softmax function denotes the dimension that the normalisation will take place:\n",
    "    - A parameter of '-1' simply refers to the last dimension of the tensor (like where -1 refers to the last element in a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3296aeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores for each of the tokens in relation to the others:\n",
      " tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "\n",
      "Norrmalised attention weights:\n",
      " tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
      "\n",
      "Context vectors for each of the tokens:\n",
      " tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# creating an empty frame for the tokens to calculate their attention scores\n",
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "# as for loops are slow, the scores can also be calculated using pytorch's matrix multiplication function\n",
    "attn_scores = inputs @ inputs.T\n",
    "\n",
    "print(\"Attention scores for each of the tokens in relation to the others:\\n\", attn_scores)\n",
    "\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "print(\"\\nNorrmalised attention weights:\\n\",attn_weights)\n",
    "\n",
    "context_vecs = attn_weights @ inputs\n",
    "\n",
    "print(\"\\nContext vectors for each of the tokens:\\n\", context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20c4c2a",
   "metadata": {},
   "source": [
    "**This completes the simplified self-attention mechanism!**\n",
    "\n",
    "- This can now be extended into the \"scaled dot-product\" self-attention mechanism used by the original transformer and GPT models\n",
    "\n",
    "- The main difference between the two is the addition of training weight matrices, which are updated during the training phase\n",
    "\n",
    "### 1. Add weight matrices to each of the tokens based on their original token embeddings\n",
    "\n",
    "- These weight matrices represent query, key, and value, where the query token will have all 3 and the other tokens will have just the key and value\n",
    "\n",
    "- Start by calculating the weights for the query token (i.e. second token for the example), and then for the rest\n",
    "\n",
    "The query, key, and value terms are taken from the domain of information retrieval and simply mean to search, store, and retrieve information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ddf1c427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query weight for the query token (token 2):\n",
      " tensor([0.4306, 1.4551])\n",
      "\n",
      "Key weight matrices for all tokens:\n",
      " tensor([[0.3669, 0.7646],\n",
      "        [0.4433, 1.1419],\n",
      "        [0.4361, 1.1156],\n",
      "        [0.2408, 0.6706],\n",
      "        [0.1827, 0.3292],\n",
      "        [0.3275, 0.9642]])\n",
      "\n",
      "Value weight matrices for all tokens:\n",
      " tensor([[0.1855, 0.8812],\n",
      "        [0.3951, 1.0037],\n",
      "        [0.3879, 0.9831],\n",
      "        [0.2393, 0.5493],\n",
      "        [0.1492, 0.3346],\n",
      "        [0.3221, 0.7863]])\n"
     ]
    }
   ],
   "source": [
    "x_2 = inputs[1] # the second token\n",
    "dim_in = inputs.shape[1] # the input dimensions for each of the tokens (3)\n",
    "dim_out = 2 # the output dimensions for each of the tokens (2)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# this assigns random values to the query, key, and value weights, based on the number of dimensions required for input and output\n",
    "W_query = torch.nn.Parameter(torch.rand(dim_in, dim_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(dim_in, dim_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(dim_in, dim_out), requires_grad=False)\n",
    "\n",
    "# matrix multiplication for the q, k, and v weights for the second token\n",
    "query_2 = x_2 @ W_query\n",
    "key_2   = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(\"Query weight for the query token (token 2):\\n\", query_2)\n",
    "\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "\n",
    "print(\"\\nKey weight matrices for all tokens:\\n\", keys)\n",
    "print(\"\\nValue weight matrices for all tokens:\\n\", values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697d6ce7",
   "metadata": {},
   "source": [
    "### 2. Compute the attention scores for each of the tokens by getting the dot product of the query and key matrices\n",
    "\n",
    "- As this example is with respect to the query token (second token), the query value used will be that of the second token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9c9eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores for all the tokens: tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n",
      "Attention weights for all the tokens: tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "# the attention scores are a matrix multiplication between the query weights and the key weights for each of the 6 tokens\n",
    "attn_scores_2 = query_2 @ keys.T\n",
    "\n",
    "print(\"Attention scores for all the tokens: \" + str(attn_scores_2))\n",
    "\n",
    "# the attention weights are calculated using this\n",
    "dim_key = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / dim_key**0.5, dim=-1)\n",
    "\n",
    "print(\"Attention weights for all the tokens: \" + str(attn_weights_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dce086",
   "metadata": {},
   "source": [
    "### 3. Compute the context vector for the query token using the attention weights of all tokens\n",
    "\n",
    "- Each of the value weights (2 dimensions) are multiplied by the attention weights (1 dimension) and summed together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13b6c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector for the second token: tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "# calculating the context vector by multiplying the attention weights and value weights\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "\n",
    "print(\"Context vector for the second token: \" + str(context_vec_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07571a0",
   "metadata": {},
   "source": [
    "### 4. Organise the code into a compact self-attention class for easy implementation\n",
    "\n",
    "- This class is derived from pytorch's nn module, which carries out the computation for inputs automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4537e6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vectors for all tokens:\n",
      " tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# the class is derived from the nn module, which has its own functions to automatically process the parameters\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(dim_in, dim_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(dim_in, dim_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(dim_in, dim_out))\n",
    "\n",
    "    # the forward function is called when inputs are given to the class\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vecs = attn_weights @ values\n",
    "\n",
    "        return context_vecs\n",
    "\n",
    "torch.manual_seed(123)\n",
    "attention = SelfAttention(dim_in, dim_out)\n",
    "\n",
    "print(\"Context vectors for all tokens:\\n\", attention(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f5142",
   "metadata": {},
   "source": [
    "### 5. Alter the self-attention class to use pytorch's native 'linear' layers\n",
    "\n",
    "- The 'linear' layer has a more optimised weight intialisation scheme, making the model more stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eff8036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vectors for all tokens:\n",
      " tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttentionNew(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        # replacing the Parameter layer with the Linear layer\n",
    "        self.W_query = nn.Linear(dim_in, dim_out,bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(dim_in, dim_out,bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(dim_in, dim_out,bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # replacing the matrix multiplication with the native linear calculation \n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vecs = attn_weights @ values\n",
    "\n",
    "        return context_vecs\n",
    "    \n",
    "torch.manual_seed(789)\n",
    "attention_new = SelfAttentionNew(dim_in, dim_out)\n",
    "\n",
    "# the new context vectors will be different to the previous ones\n",
    "print(\"Context vectors for all tokens:\\n\", attention_new(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43efeff2",
   "metadata": {},
   "source": [
    "**This completes the self-attention mechanism used in many LLMs and GPT models!**\n",
    "\n",
    "- This can now be extended into a 'causal' self-attention mechanism, which is a specialised form of self-attention\n",
    "\n",
    "    - Instead of considering the entire input sequence at once, this model only considers current and previous tokens\n",
    "\n",
    "    - The tokens ahead of the current token are masked and normalise the attention weights of the unmasked tokens\n",
    "\n",
    "### 1. Mask the computed attention weights for the inputs (6 tokens)\n",
    "\n",
    "- The attention weights are first masked accordingly, and normalised such that the unmasked ones add up to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfd2b9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights for the inputs:\n",
      " tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Masked tokens for each token:\n",
      " tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "\n",
      "Normalised masked tokens for each token:\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# compute the attention scores, then attention weights from the query and key weights\n",
    "queries = attention_new.W_query(inputs)\n",
    "keys = attention_new.W_key(inputs)\n",
    "\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "print(\"Attention weights for the inputs:\\n\", attn_weights)\n",
    "\n",
    "# for each token, mask the values of future tokens (i.e. for token 1, mask tokens 2 - 6/ for token 2, mask tokens 3 - 6, etc.)\n",
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "\n",
    "print(\"\\nMasked tokens for each token:\\n\", mask_simple)\n",
    "\n",
    "# normalise the tokens so that all unmasked tokens in each level add up to 1\n",
    "\n",
    "mask_simple = attn_weights * mask_simple\n",
    "row_sums = mask_simple.sum(dim=-1, keepdim=True)\n",
    "norm_mask_simple = mask_simple / row_sums\n",
    "\n",
    "print(\"\\nNormalised masked tokens for each token:\\n\", norm_mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab397c1",
   "metadata": {},
   "source": [
    "### 2. Use a 'dropout' technique to ignore some attention weights\n",
    "\n",
    "- This prevents LLMs from being overly reliant on any set of attention weight units during training\n",
    "\n",
    "- The following example shows how dropouts are applied on attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67050cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example before dropout:\n",
      " tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "\n",
      "Example after dropout:\n",
      " tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n",
      "\n",
      "Dropout used on previous attention weights:\n",
      " tensor([[0.3843, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3324, 0.0000, 0.3329, 0.2955],\n",
      "        [0.0000, 0.3318, 0.3325, 0.2996, 0.3328, 0.2961],\n",
      "        [0.0000, 0.0000, 0.3337, 0.3142, 0.0000, 0.3128],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3317, 0.3169],\n",
      "        [0.3869, 0.3327, 0.0000, 0.3084, 0.3331, 0.3058]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# using a dropout factor of 0.5 makes the model ignore half of the weights in each batch (i.e out of 6 weights, only 3 are considered)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "\n",
    "# the dropout reduces some weights to 0, and multiplies the remaining weights by a scale (i.e. 1 / 0.5 = 2)\n",
    "print(\"Example before dropout:\\n\", example)\n",
    "print(\"\\nExample after dropout:\\n\", dropout(example))\n",
    "print(\"\\nDropout used on previous attention weights:\\n\", dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1dfe70",
   "metadata": {},
   "source": [
    "### 3. Incorporate the previous steps into a compact causal attention class\n",
    "\n",
    "- This class is similar to the previous self-attention class but incorporates the masking and dropout steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a040237d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vectors for 2 batches of 6 tokens:\n",
      " tensor([[[-0.6053,  0.6575],\n",
      "         [-0.5503,  0.6564],\n",
      "         [-0.5313,  0.6538],\n",
      "         [-0.4500,  0.5680],\n",
      "         [-0.4254,  0.5241],\n",
      "         [-0.3933,  0.5011]],\n",
      "\n",
      "        [[-0.6053,  0.6575],\n",
      "         [-0.5503,  0.6564],\n",
      "         [-0.5313,  0.6538],\n",
      "         [-0.4500,  0.5680],\n",
      "         [-0.4254,  0.5241],\n",
      "         [-0.3933,  0.5011]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class CasualAttention(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.dim_out = dim_out\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "        self.W_query = nn.Linear(dim_in, dim_out,bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(dim_in, dim_out,bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(dim_in, dim_out,bias=qkv_bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch, num_tokens, dim_in = x.shape\n",
    "        \n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vecs = attn_weights @ values\n",
    "\n",
    "        return context_vecs\n",
    "\n",
    "# duplicating the inputs to show that the casual attention module can handle 2 batches at once\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "context_length = batch.shape[1]\n",
    "casual_attention = CasualAttention(dim_in, dim_out, context_length, 0.0)\n",
    "context_vecs = casual_attention(batch)\n",
    "\n",
    "print(\"Context vectors for 2 batches of 6 tokens:\\n\", context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c60b453",
   "metadata": {},
   "source": [
    "**This completes the causal self-attention mechanism!**\n",
    "\n",
    "- This can finally be extended into a multi-head attention mechanism, which employs the causal attention over multiple \"heads\"\n",
    "\n",
    "- A single head refers to only one set of attention weights being processed by the LLM, multiple heads = multiple weights\n",
    "\n",
    "### 1. Create a wrapper class that runs multiple instances of the causal attention class\n",
    "\n",
    "- Each head returns a single context vector for each token input, where the context vectors are concatenated and returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda67edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-head context vectors for 2 batches of 6 tokens:\n",
      " tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([CasualAttention(dim_in, dim_out, context_length, dropout, qkv_bias)\n",
    "                                   for _ in range(num_heads)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "    \n",
    "# using this class on 2 instances of the causal attention class would return a final context vector with 4 dimensions\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # number of tokens is 6 for each batch\n",
    "dim_in, dim_out = 3, 2\n",
    "\n",
    "multi_head = MultiHeadAttentionWrapper(dim_in, dim_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = multi_head(batch)\n",
    "\n",
    "print(\"Multi-head context vectors for 2 batches of 6 tokens:\\n\", context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70053c5f",
   "metadata": {},
   "source": [
    "### 2. Combine the casual attention class and multi head wrapper into a single class\n",
    "\n",
    "- The previous multi head attention class simply instantiates and combines several causal attention objects\n",
    "\n",
    "- This can be made more efficient by splitting the input into multiple heads by reshaping the projected q, k, v weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e1f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (dim_out % num_heads == 0), \\\n",
    "            \"dim_out must be divisible by num_heads\"\n",
    "    \n",
    "        self.dim_out = dim_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(dim_in, dim_out,bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(dim_in, dim_out,bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(dim_in, dim_out,bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(dim_in, dim_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, num_tokens, dim_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vecs = (attn_weights @ values).transpose(1, 2)\n",
    "        context_vecs = context_vecs.contiguous().view(batch, num_tokens, self.dim_out)\n",
    "        context_vecs = self.out_proj(context_vecs)\n",
    "\n",
    "        return context_vecs\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
